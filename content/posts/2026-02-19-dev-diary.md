---
title: "The Apple Display That Only Speaks Thunderbolt"
date: 2026-02-19T20:56:48-07:00
description: "Launching CureCancerMagic, completing the AuthorMagic book publishing pipeline, overhauling demo mode, and discovering that the Apple Studio Display refuses to take HDMI from a Raspberry Pi."
draft: true
tags: ["dev-diary", "claude-code", "nextjs", "supabase", "deployment"]
---

Late in the day I plugged a Raspberry Pi 5 into the Apple Studio Display using a micro-HDMI to USB-C cable and got nothing. No splash screen, no error, just a monitor that didn't react. After some debugging I found the reason: the Studio Display only accepts Thunderbolt 3 video input. It cannot receive HDMI. The USB-C Digital AV Multiport Adapter - the one Apple sells for connecting Macs to HDMI TVs - is a one-way converter. It sends Mac video OUT to HDMI devices. There is no path from a Raspberry Pi's micro-HDMI output to a Studio Display input. The solution, I thought, was going headless - SSH from the Mac on the same network. But the Pi I bought had all the ports locked down in the configuration, so SSH wasn't an option either. Amazon is bringing me a 7 Inch IPS LCD Touch Screen Raspberry Pi Monitor Display tomorrow morning.

The rest of the day was software.

---

## Things We Learned Today

The demo mode work for AuthorMagic - a book management platform for authors - produced the most useful single insight of the day. The original design called for dedicated "demo accounts" with special mechanisms: discount code redemption to grant subscription tiers, admin override tables, promote and clear buttons, lifecycle tracking through launch cohorts. Around 200 lines of infrastructure for simulating what an alpha user experiences.

Then I looked at actual alpha users in production. They're just regular users. Same auth flow, same database rows, same RLS policies. The only thing special about them is they're in a launch cohort and have a tag.

So the demo "New User" flow became: `auth.admin.createUser()` to create a real user, update `shared.users` with a name, add them to a cohort, then use god mode's existing cookie-based impersonation to switch to that user. That came to about 50 lines. Every feature works because the app doesn't know the admin created the account - to the app it IS a real user session.

The lesson I wrote down: before building a mechanism to simulate X, check whether you can just do X directly.

That said, getting there required understanding why the first approach failed. Demo accounts had books seeded with the `is_demo_book` flag set to `true`, and sales records with `is_seed_data` set to `true`. Every major service in the codebase filters those flags out - `book.service.ts`, analytics, spike detection, correlation analysis. This is correct behavior for real users who generated sample data during onboarding. It is catastrophic for a demo account whose only data is that seed data. The fix was to set the flags to `false` for dedicated demo accounts so the data is treated as "real." I found the same bug filtering `is_seed_data` in 17 files via grep.

The deeper pattern: when a boolean flag exists to separate real data from synthetic data, a demo mode that uses synthetic-as-real data must audit every filter in the query chain.

---

Configuration drift in monorepos follows a pattern I kept running into. When CureCancerMagic - a cancer care coordination app - was added to the platform, the "big" items got updated correctly: Vercel project registry, deployment config, Supabase schema. But 10+ ancillary configurations were missed: CI test matrix entries, DAST security scanning lists, secrets validation scripts, backup scripts. The CI `APP_MAP` omission meant CureCancerMagic PRs didn't trigger test detection for that app - a live gap that went unnoticed.

The structural reason this happens: the `APPS` record in `packages/config/src/apps.ts` uses TypeScript's `Record<AppId, ...>` pattern, so adding a new `AppId` creates compile errors everywhere a per-app config is missing that entry. The type system enforces the central record. But the 20+ ancillary configs are scattered across shell scripts, YAML arrays, and JSON files that the type system can't touch. Those require operational discipline - specifically, a checklist. The runbook created today organizes the full process into 6 sequential phases with checkboxes. The deeper insight: monorepo consistency is a spectrum. Some invariants can be compiler-enforced, others require humans with lists.

---

Context window economics turned out to be an interesting design problem. About 46,000 tokens are consumed before any user interaction in a Claude Code session on this codebase - roughly 23% of the 200K window. When `/start` and `/commit` run in a session, that jumps to around 72,000 tokens (36%). The most actionable finding: the `on-demand-references.md` pattern (a 620-byte index pointing to large docs that load only when needed) is already the gold standard in the codebase - it's just not applied consistently. Moving 8 rarely-needed rules to on-demand references saves ~6,500 tokens with zero behavioral change.

The deferred MCP tool listing alone consumes ~5,000 tokens just for tool names - 388 tools across 12 servers.

---

React 19's `useTransition` has a subtle error propagation issue with server actions. When a server action throws inside `startTransition(async () => { try { await serverAction() } catch {} })`, the error doesn't always get caught by the try/catch - it can propagate to the nearest Error Boundary instead, leaving `isPending` stuck as `true` forever. The symptom is a button that spins indefinitely. The fix is manual loading state: `useState(false)`, `setLoading(true)` before try, `toast.success/error` for feedback, `finally { setLoading(false) }`. This gives complete control over the loading lifecycle. `useTransition` is designed for non-urgent UI transitions, not server action error handling.

---

One more, on Sentry: `ignoreErrors` is processed by the SDK's `eventFilters` integration before `beforeSend` fires. String entries match via `String.includes()` against the combined error string. So `"AbortError"` catches `"AbortError: signal is aborted without reason"` and any future variant automatically, with zero per-event processing cost. `beforeSend` is for context-dependent filtering that examines stack frames or event metadata. Edge configs in this codebase had no `ignoreErrors` arrays at all - adding one is purely additive since the SDK merges it with its internal default list.

---

## Things We Did Today

CureCancerMagic - a cancer care coordination app - went from schema and initial landing pages to a working auth flow, dashboard navigation, and unit test coverage. The RLS pattern is fundamentally different from other platform apps: instead of user-scoped policies (`user_id = auth.uid()`), it's case-scoped, where every table's policy checks membership in `care_team_members` for the active case. This enables multi-user collaboration on a single patient's care record. The `isConsumerApp: false` flag in the platform config is a deliberate choice - users aren't self-acquired through marketing funnels, they're invited by case coordinators.

Getting it running locally produced a few interesting bugs. Next.js 16 enforces that `cookies().set()` can only happen in mutation contexts (form submissions, server actions called from client interactions) - not during Server Component rendering. The `case-context.ts` function had a cookie write on the read path that worked fine under the old rules but throws in 16. Fix: remove the write from the read path entirely, let it only happen through explicit mutation paths like `setActiveCaseId()`. Related: the `noAccessUrl` in middleware must point to a page that won't redirect authenticated users away. `"/sign-up"` caused an infinite redirect loop for users already authenticated through another app on the same Supabase instance. Changed it to `"/onboarding"` with a repair function that creates the missing `user_app_access` record before completing onboarding.

---

The AuthorMagic book website publishing pipeline reached end-to-end today. The full chain: author editing landing page config in AuthorMagic → adapter function translating 24 AuthorMagic-specific section types to the platform's 11-type `SiteConfig` → TOML generation for Hugo → 25 Hugo partials rendering each section → per-author Vercel project deploy. The adapter uses a deliberate 6:18 split: 6 known mappings (hero, about, reviews to testimonials, faq, gallery, contact) that transform field names to platform-standard shapes, and 18 custom passthrough mappings where the full JSONB data flows through with only a `sectionType` discriminator added for Hugo to dispatch on. Adding a new AuthorMagic section type requires zero adapter changes - it flows through as `type: "custom"` automatically.

The integration test caught a bug the unit tests missed: the slug `"/"` in the adapter output versus the `"home"` convention that the validation layer expects. The adapter tests passed because they tested the adapter against the type contract, not against the runtime validator. This is a clean integration gap.

---

The review triage system got an overhaul. The old logic used file count as the primary signal: fewer than 3 files meant NONE (no review). One file could have a security hole. Three harmless file renames got reviewed more carefully than a single dangerous server action change.

The new logic: content type is the discriminator. NONE applies only when ALL changed files are non-source (docs, tests, config, styling). Any `.ts` or `.tsx` source file gets at least LIGHT review. LIGHT is now multi-agent on Sonnet - `code-reviewer` plus `ui-consistency-reviewer` for tsx files, plus `silent-failure-hunter` for API routes, services, actions, and hooks, all dispatched in parallel. FULL adds `security-auditor`, `test-quality-reviewer`, and `spec-reviewer` for larger or sensitive changesets. Parallel dispatch makes multi-agent LIGHT essentially free in wall-clock time. The only cost is tokens.

---

Other things that shipped: email infrastructure consolidated across the platform (five apps sharing one `sendEmail()` call instead of local implementations in each), AbortError noise filtered from Sentry across all six instrumentation files, rankings-sync batch size made configurable by environment to handle Vercel's proxy idle timeout in Preview, pre-commit hook fixed to handle files reached through symlinked directories (not just files that are directly symlinks), and the Freshell open-source project got a deep code review pass.

On the Raspberry Pi front (OVR-10): the green activity LED is the key diagnostic - blinking means the OS was found and boot is in progress, no blink at all means no bootable media. That distinction alone would have saved me 20 minutes at the start. The monitor arrives tomorrow.

---

## Fun Things to Try

Linear's GraphQL API has an `emailIntakeAddressCreate` mutation that the MCP server doesn't expose. Pass a `teamId` and Linear generates a `@linear.app` intake address. There's no list query for these in the public schema - only single-record lookup by ID - so store the IDs returned at creation time if you'll need to manage them later. This is worth knowing for any Linear feature that isn't surfaced through MCP tools.

---

The review ecosystem audit (comparing custom review agents against commercial tools like CodeRabbit and Greptile) surfaced an interesting architecture improvement: inter-agent discourse. Right now the review agents run in parallel and their findings get concatenated. When `code-reviewer` flags "missing error handling on line 45" and `silent-failure-hunter` flags "swallowed error on line 43," they're reporting related issues independently. A synthesis pass that reads all agent outputs and merges related findings into single actionable items would reduce noise considerably. I'd implement it as a general-purpose subagent at the end of the review chain rather than a custom agent - deduplication is orchestration, not domain-specific review.

---

Discourse's theme component architecture is cleaner than I expected. Built-in themes (negative IDs like -1 Foundation, -2 Horizon) can't be modified via the REST API but color schemes can - so you get palette control through the API without touching CSS. Custom theme components (positive IDs) are the right customization layer: SCSS that loads after the parent theme's styles. The word-matching selector pattern is especially clever: `[data-effects~="dropcaps"]` activates when the space-separated attribute contains that word. Adding a new effect is a CSS block plus a new word in the attribute, no JavaScript required. The inverted naming convention (`primary` = text color, `secondary` = background color) is the one thing to write down before you go banging your head against it.

---

*Generated from daily notes. Review and edit before publishing.*
